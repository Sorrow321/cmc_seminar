Задача - построение чат бота для торговых центров. Реализуемая фича - поиск по товарам с помощью запроса, заданного на естественном языке.

**22.10.2020** Примерно получили представление, что нужно для построения нужной нам системы. Пока остановились на том, что нам нужна модель ранжирования. Создали некоторый baseline без использования DL (на основе обычного bag of words и меры Жаккара). Соответственно ближайший шаг - сбор тестовой выборки и evaluation модели для сравнения с дальнейшими улучшениями базовой модели. Направление дальнейшего исследования - DL для NLP в задаче ранжирования, с учетом особенностей нашей задачи. В частности, нужно учитывать то, что в нашей задаче объектами для ранжирования служат товары, а не чистые тексты, а также то, что мы пока будем решать эту задачу как unsupervised, то есть у нас отстутствует разметка, которая обычно есть в задачах ранжирования.


**1.11.2020** Поучаствовал в хакатоне Лидеры Цифровой Трансформации https://hack2020.innoagency.ru/ по треку Разработка умного правового помощника для предпринимателей.

**2.11.2020** Занял 1 место в вышеупомянутом хакатоне. Приз - 1 млн рублей. Задача была: Разработка умного правового помощника для предпринимателей.<br>
**30.11.2020** Занял 2 место в соревновании по Data Science от архипелага 20.35 https://online.innoagency.ru/datascience/. Приз - 100к рублей. Задача: Разработка алгоритма на основе искусственного интеллекта для автоматизированной оценки комплектности и качества неструктурированного (в т.ч. визуального) содержимого документов, подаваемых в составе заявки на меры государственной поддержки

**14.02.2021** Готовлюсь к пересдаче по БММО до 18-го февраля, поэтому пока не могу активно заняться дипломной работой (активно включусь сразу же как сдам)

**21.02.2021** Начинаю активно заниматься дипломом. Нашел соревнование: https://boosters.pro/championship/data_fusion/overview во второй задаче которого нужно находить бренд товара в текстовой строчке. Как мне кажется, это отличная подзадача для задачи построения поисковой системы в интернет магазине. Поэтому я буду решать это соревнование, искать и изучать современные подходы к решению такого рода задач, и на основе полученных знаний и проведенных экспериментов создавать основу для поиска. Составлю некоторый план работы (над дипломом в целом) с дедлайнами для самого себя, чтобы не потеряться во времени. Опубликую его сюда же.

**22.02.2021** Обговорили с Алексеем дальнейшее сотрудничество. Как оказалось на практике, чат-ботом для ТЦ пользуется довольно мало людей (порядка десятка в день), поэтому провести качественные A/B тестирования различных гипотез будет проблематично. Поэтому решили так: я независимо веду разработку поиска, а потом мы его внедряем в чат-бота (без AB тестов). Ориентировочная дата, к которой должен быть готов пилотный проект - конец марта 2021 года. 

**01.02.2021** Посмотрел лекцию из CS224n по LM и RNN, разобрался в алгоритме Backpropagation through time (backprop для RNN).

**06.02.2021** Изучил GRU и LSTM тут http://d2l.ai/chapter_recurrent-modern/lstm.html (очень хороший онлайн-учебник по дип лернингу)

**07.02.2021** Приступил к соревнованию, пока тестирую NER на основе трансформера.

**08.02.2021** Разметка в соревновании просто ужасная. ![image](https://user-images.githubusercontent.com/20703486/110515749-c7222000-8119-11eb-899b-c604c5668c4f.png) <- тут очевидно бренд campina fruttis, а в лейбле стоит просто fruttis. И так примеров куча...

**09.02.2021** Весь день сидел на tokenizer'ом от HuggingFace, пытался понять что он делает и почему жрет 60 гигов ОЗУ.... В итоге вроде как придется токенизировать по батчам, иначе с трудом влезает в мои 64 гига озу (а на соревновании решение запускается у них на сервере в докере, в котором дается 46 Гб)

**10.02.2021** Наконец-то завел на NER RuBERT через HuggingFace. Правда, учится это все очень долго... 1 эпоха на моей видеокарте будет идти порядка 4 часов. Буду думать, где бы обучить по-быстрее....![image](https://user-images.githubusercontent.com/20703486/110581347-65929d80-817b-11eb-82da-714b24130ec1.png)

**11.02.2021** Обучил за 8 часов 2 эпохи. В качестве претрейненной модели взял RuBERT от Deep Pavlov (через HugginFace).

Проблема номер 1: как оказалось, в соревновании в качестве решения можно отправлять архивы размером <= 500 мегабайт. Однако мой обученный берт занимает порядка 700 мегабайт. Если сжимать веса в zip архив, выйдет порядка 650 мегабайт... Казалось бы, проблему можноо решить просто взял берта меньше (small версию), но ок584650азыавается, что таких бертов, обученных на русский текст, банально нет (либо я пока недостаточно хорошо гуглил). Мои идеи, что делать: либо предобучать свой маленький берт (скорей всего у меня на это не будет ресурсов, потому что берт предобучается на космическом количестве данных), либо гуглить лучше, чтобы найти RuBERT с меньшим числом параметром, либо брать мультиязычные версии берта (не уверен, что они также бывают маленькие), либо отходиь от берта и пытаться решать задачу другими архитектурами.

Проблема номер 2. Пока что не получилось нормально сделать инференс на обученной модельке. Возможнно, это потому что сейчас 5 утра. Попробую продолжить завтра.

Первая проблема меня прямо сильно напрягает, потому что я планировал брать берта в качестве бейзлайна и мне прямо совсем не хочется уходить на другую модель. В худшем случае придется забить на соревнование и работать сразу над моделью для диплома. В частности, надо будет придумать, как сгенерировать данные (либо как их размечать).

Также, в принципе у меня выстроилась основная идея диплома - это построение NER для поисковых запросах. То есть человек задает запрос, я прогоняю его через NER, выделяю ключевые фичи, строю признаковое описание, а дальше выполняю уже линейный поиск по базе товаров в магазинах, которые я спаршу с сайтов. Соответственно в дипломе будет описана модель, фичи, а также тонкости, свзанные с конкретной задачей (как улучшить общую задачу NER под конкретно этот случай). Мне кажется, получится неплохо, однако время уже поджимает...
