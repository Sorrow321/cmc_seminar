\documentclass[12pt,fleqn]{article}

\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb,amsmath,mathrsfs,amsthm}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}

%\usepackage[ruled,section]{algorithm}
%\usepackage[noend]{algorithmic}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-1cm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.5} %для печати с большим интервалом
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays

\newtheorem{obs}{Наблюдение}
\newtheorem{conc}{Вывод}
\newtheorem{defin}{Определение}
\newtheorem{example}{Пример}
\newcommand{\Norm}[1]{\left\lVert#1\right\rVert}

\begin{document}

\begin{titlepage}
\begin{center}
    Московский государственный университет имени М. В. Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        КУРСОВАЯ РАБОТА СТУДЕНТА 317 ГРУППЫ\\[10mm]
        <<Сравнительный анализ методов быстрого поиска ближайших соседей>>
    }\\[10mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 3 курса 317 группы\\
            \emph{Федоров Илья Сергеевич}\\[5mm]
            Научный руководитель:\\
            д.ф-м.н., профессор\\
            \emph{Дьяконов Александр Геннадьевич}
        }
    \end{flushright}

    \begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
        Заведующий кафедрой\newline
        Математических Методов\newline
        Прогнозирования, академик РАН
        &
        ~\newline~\newline
        \hfill\hbox to 0.45\textwidth{\hrulefill~Ю. И. Журавлёв}
    \\[20mm]
        К защите допускаю\newline
        \hbox to 0.4\textwidth{<<\hbox to 12mm{\hrulefill}>> \hrulefill~2020 г.}
        &
        К защите рекомендую\newline
        \hbox to 0.45\textwidth{<<\hbox to 12mm{\hrulefill}>> \hrulefill~2020 г.}
    \end{tabular}

    \vspace{\fill}
    Москва, 2020
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

\newpage
\begin{abstract}
    Todo
\end{abstract}

\newpage
\section{Введение}

Одним из наиболее простых и естественных методов машинного обучения является метод ближайшего соседа. Имея набор данных, представленных в виде точек в некотором многомерном пространстве, целевая величина (будь то класс или вещественное число) прогнозируется по значениям отклика на k ближайших к запросу точках из исходного набора данных. В то время как существует множество различных подходов к усреднению данных k значений, наиболее вычислительно затратной частью алгоритмов подобного типа является именно поиск ближайших соседей. Действительно, в современных задачах объемы данных достигают колоссальных размеров, что делает алгоритмы, основанные на полном переборе, неэффективными. Задача поиска ближайших к запросу точек в некотором наборе данных встречается не только в задачах прогнозирования. Примерами приложений также могут служить задачи поиска дубликатов в больших объемах данных (или «почти» дубликатов), поиска похожих изображений и текстов. Целью данной работы является обзор современных подходов к решению задачи поиска ближайших соседей и её вариаций, а также сравнительный анализ эффективности тех или иных методов её решения в зависимости от особенностей пространства, в котором расположены данных. В исследовании представлены как классические подходы, основанные на формировании некоторых дополнительных структур данных, так и наиболее современные «приближенные» методы.


\subsection{Определения и обозначения}

Формализуем постановку задачи. Основным объектом нашего изучения будет пространство признаков вместе с функцией расстояния  $\mathbb{X} = \left(\mathbb{R}^n, d \right)$. Важно заметить, что функция $d$ в приложениях довольно часто может не удовлетворять формальному определению метрики, однако даже в этом случае в данной работе подобные функции, допуская некоторую вольность, будут называться метриками. К примеру, широко используемое в анализе текстов косинусное расстояние $d(x, y) = 1 -\frac{\langle x, y \rangle}{\norm{x} \norm{y}}$ не удовлетворяет неравенству треугольника. В данной работе в большинстве случаев будет использоваться евклидовая метрика и описанное выше косинусное расстояние. Будем обозначать $X \in \mathbb{R}^{l \times n}$ матрицу для выборки точек из $\mathbb{X}$, где строки соответствуют объектам, а столбцы признаками. Формально задача поиска k ближайших соседей ставится следующим образом: имея множество объектов $X$ из пространства $\mathbb{X}$ и запрос $q \in \mathbb{X}$, нужно найти в $X$ k ближайших к $q$ точек по метрике $d$. Более подробно, если посчитать расстояния между $q$ и всеми объектами из $X$, а потом расположить их в отсортированном порядке
$$d(x_{i_1}, q) \leq d(x_{i_2}, q) \leq \dotso \leq d(x_{i_l}, q),$$
то алгоритм должен выдать k объектов с минимальными расстояниями: $x_{i_1}, x_{i_2}, \dotso, x_{i_k}$.

Как мы увидим далее, большинство современных методов быстрого поиска ближайших соседей на самом деле решают описанную выше задачу с некоторыми ослаблениями, что в сущности приводит к задаче «приближенного» поиска ближайших соседей. Эта задача не имеет общепризнанной формальной постановки, разные авторы могут по-разному понимать её. К примеру, довольно распространенной постановкой задачи является следующая формулировка: имея набор точек $X$ из $\mathbb{X}$, запрос $q \in \mathbb{X}$, а также параметр $c \geq 1$, если существует точка $x \in X$, такая что $d(x, q) \leq r$, алгоритм должен вернуть точку $x^* \in X$, такую что $d(x^*, q) \leq cr$. В дальнейшем при описании конкретных методов, мы будем уточнять, какую именно задачу приближенного поиска ближайших соседей они решают.

\subsection{Обзор литературы}

todo, напишу после основной части

\section{Обзор существующих методов поиска ближайших соседей}

\subsection{Прямое вычисление матрицы попарных расстояний}

Самым простым и распространенным способом поиска ближайших соседей является прямой перебор. Имея набор данных $X$ и запрос $q$, мы вычисляем расстояния между каждым объектом $x \in X$ и $q$. После этого полученные расстояния сортируются, и алгоритм выдает $k$ объектов из $X$, имеющих наименьшие расстояния до $q$.

Оценим вычислительную сложность такого алгоритма. Заметим, что для вычисления евклидового или косинусного расстояния между двумя объектами размерности $n$ нужно совершить порядка $n$ операций. Отсюда получаем, что сложность вычисления расстояний $\mathcal{O}(nl)$ (в наших обозначениях $l$ -- число объектов). Далее требуется отсортировать полученный массив, что займет $\mathcal{O}(l \log(l))$ операций. Наконец, останется совершить $\mathcal{O}(k)$ операций для выдачи результата. Учитывая, что $k \leq l$, получим итоговую сложность: $\mathcal{O}(nl + l \log(l))$. Однако на практике данную сложность можно улучшить. Дело в том, что обычно $k \ll l$, а значит б\'ольшая часть информации из отсортированного массива нам не нужна. Поэтому вместо сортировки можно использовать более эффективные алгоритмы для поиска $k$ наименьших чисел в массиве. Например, это можно сделать с помощью структуры данных под названием куча. Имея массив из $l$ элементов, можно построить кучу за $\mathcal{O}(l)$, а далее вытащить из неё $k$ минимальных элементов за $\mathcal{O}(\log(l))$ каждый. Получаем сложность поиска $k$ минимальных чисел в массиве $\mathcal{O}(l + k \log(l))$. Учитывая, что $k \ll l$, вторым слагаемым можно пренебречь, и оценить итоговую сложность всего алгоритма в $\mathcal{O}(nl)$.

Данный алгоритм вполне эффективен и применим, если объем данных и запросов не слишком велик. К примеру, в соревнованиях по машинному обучению довольно часто встречаются наборы данных размером порядка $10^5 - 10^6$ размерности около 100. Имея обучающую выборку размером $10^6$ размерности $100$, а также тестовую выборку размером $2 \times 10^6$, алгоритм прямого перебора будет работать около 13 часов на 8-ядерном процессоре. Это вполне приемлимо, если требуется решить задачу для конкретной тестовой выборки. Однако данный алгоритм обладает рядом существенных недостатков. Во-первых, если поиск ближайших соседей проводится для решения какой-то задачи машинного обучения, то все вычисления проводятся непосредственно в момент предсказания целевой величины. Поскольку алгоритм никак не обучается, его ценность с точки зрения производительности существенно падает, так как при каждом предсказании производится набор вычислений, сопоставимый по объему с обучением какого-то другого алгоритма, который, обучившись лишь единожды, может очень быстро выдавать ответы (например, линейная или логистическая регрессия). Во-вторых, если данных становится действительно много (скажем, больше $10^{10}$), то для хоть сколько-то большой тестовой выборки уже требуется колоссальное количество времени для вычислений. Современные компьютеры могут выполнять примерно $10^8$ операций в секунду, поэтому для обучающей выборки размером $10^{10}$ (вполне реальная цифра для больших компаний), тестовой выборки размером $10^3$, размерности пространства $10$ такое вычисление займет $\frac{10^{10} \times 10^3 \times 10}{10^8}$ секунд $\approx 278$ часов $\approx 12$ дней. Безусловно, эти цифры можно сократить, используя специализированные архитектуры компьютеров и многопоточность, однако представленные два недостатка данного алгоритма в совокупности ставят под сомнение его использование в промышленных масштабах.

\subsection{Древовидные структуры данных}

Большой класс алгоритмов для быстрого поиска ближайших соседов основан на идее разбиения признакового пространства на области, которые объединяются в различные структуры данных, позволяющие выполнять поиск ближайших соседей для новых запросов быстрее.	

\begin{figure}[h]
	\begin{center}
		{\includegraphics[scale=0.2]{r_tree_pic.png}}
	\end{center}
	\caption{Пример разбиения данных на плоскости с помощью R-Tree}
	\label{fig:image}
\end{figure}

Большинство алгоритмов из данного класса сначала осуществляют предварительную обработку исходного набора данных, строя древовидные структуры, состоящие из областей разбиений. Далее, при поступлении очередного запроса, используется информация, полученная на первом этапе. Таким образом, важное отличие таких алгоритмов от метода прямого перебора заключается в том, что теперь мы имеем некоторый разделенный интерфейс, состоящий из двух методов: построение дерева и запрос. Это позволяет нам по отдельности оценивать вычислительные сложности для этих двух операций. Это может иметь важную роль, к примеру, если в решаемой практической задаче не так существенно, сколько займет первичная обработка данных, но требуется высокая скорость обработки новых запросов.

Перечислим наиболее популярные алгоритмы поиска ближайших соседей, основанные на древовидных структурах данных:

\begin{center}
	\begin{multicols}{2}
		\begin{itemize}
			\item KD - Tree
			\item Ball - Tree
			\item R - Tree
			\item BSP - Tree
			\item Quadtree
			\item B - Tree
		\end{itemize}
	\end{multicols}
\end{center}


\noindent Стоит отметить, что некоторые из этих структур данных предназначенны для работы с данными какой-то фиксированной размерности. Например, B - Tree работает для одномерных данных, а Quadtree -- для двумерных. На практике наиболее часто встречаются алгоритмы KD - Tree и Ball - Tree, поскольку они включены в самые известные библиотеки для машинного обучения. Рассмотрим в качестве примера подробную реализацию KD - Tree.


Приведем возможную реализацию KD - Tree на псевдокоде. Сначала рассмотрим операцию построения дерева. 


\begin{algorithm} 
	\begin{algorithmic}[1]
		\Procedure{BuildNode}{$\Omega$}
		\If{$|\Omega| \leq n_{min}$}
		\State self.objects = $\Omega$
		\Else
		\State self.pivot\_feature\_idx = $\argmax\limits_{1 \leq i \leq n} \mathbb{D}[x^i]$ 
		\State self.threshold = median($x^{self.pivot\_feature\_idx}$)
		\State self.left = BuildNode(\{ $x_k \in \Omega \enspace | \enspace x_k^{self.pivot\_feature\_idx} < self.threshold$  \})
		\State self.right = BuildNode(\{ $x_k \in \Omega \enspace | \enspace x_k^{self.pivot\_feature\_idx} \geq self.threshold$  \})
		\EndIf
		\EndProcedure
		
	\end{algorithmic}
\end{algorithm}

\noindent Таким образом, метод разбивает набор данных по медиане признака с наибольшей дисперсией на две части и рекурсивно применяется к каждой из них. Полученные деревья считаются сыновьями данной вершины. Рекурсия прекращается, когда набор данных становится достаточно небольшим по размеру. Отметим, что различных источниках можно найти немного отличающиеся реализации данной операции, однако приведенный вариант хорош тем, что приводит к достаточно сбалансированному дереву. 

\begin{figure}[h]
	\begin{center}
		{\includegraphics[scale=0.6]{kd_tree.png}}
	\end{center}
	\caption{Пример построенного KD-Tree, $n_{min} = 3$}
	\label{fig:image}
\end{figure}

Операция запроса в KD - деревьях работает по следующему принципу: сначала устанавливается лист, соответствующей области разбиения, содержащей запрос. Вычисляется ближайший к запросу сосед среди точек в этом листе. Далее начинается восходящий по структуре дерева поиск ближайших соседей в соседних областях. А именно, если расстояние от запроса к прямоугольнику, который является другим сыном родителя листа, в котором находится запрос, меньше, чем расстояние до текущего ближайшего соседа, то алгоритм проверяет эту область на наличие ещё более близких соседей. Далее алгоритм поднимается на одну вершину вверх по дереву и выполняет те же действия. На рисунке 3 представлена иллюстрация к описанному алгоритму. Красная и синияя области -- это листья в KD - дереве, черная область -- их родитель, зеленые точки -- исходный набор данных, черная точка - запрос. Ближайшей точкой в красной области является точка под номером 1, однако точка 3 находится ближе к запросу, поэтому алгоритм проверяет «братские» области к тем, в которых расположен запрос.

\begin{figure}[h]
	\begin{center}
		{\includegraphics[scale=0.6]{kd_query.png}}
	\end{center}
	\caption{Иллюстрация к операции запроса в KD - дереве}
	\label{fig:image}
\end{figure}

\noindent Приведем реализацию операциии запроса на псевдокоде. Данный код был взят из \cite{kitov}

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\Procedure{MakeQuery}{Tree, Query}
		\State // Поиск листа
		\State CURRENT\_NODE = root node of Tree
		\While {CURRENT\_NODE is not leaf node}
		\State $pivot = Query^{CURRENT\_NODE.pivot\_feature\_idx}$
		\State $\mu$ = CURRENT\_NODE.threshold
		\If {$pivot \leq \mu$}
		\State CURRENT\_NODE = CURRENT\_NODE.left
		\Else
		\State CURRENT\_NODE = CURRENT\_NODE.right
		\EndIf
		\State ascendant\_search(CURRENT\_NODE)
		\EndWhile
		
		\EndProcedure
		
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\begin{algorithmic}[1]
			\Procedure{ascendant\_search}{CURRENT\_NODE}
			\State // Восходящий поиск
	
			\State mark CURRENT\_NODE as checked
			
			\While{not all nodes of Tree checked}
			\State SIBLING\_NODE = brother node of CURRENT\_NODE
			\State RECT\_DIST = distance from Query to rectangle, associated with SIBLING\_NODE
			\If{RECT\_DIST $\geq$ NN\_DIST}
			\State mark SIBLING\_NODE and all its descendants as checked
			\Else
			\State NN,NN\_DIST = check\_tree(SIBLING\_NODE)
			\EndIf
			
			\State mark SIBLING\_NODE and PARRENT\_NODE as checked
			\State set CURRENT\_NODE to PARENT\_NODE
			\EndWhile
	
			\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\Procedure{check\_tree}{CurrentNode, x, NN, NN\_Dist}
				\If {CURRENT\_NODE is leaf node}
					\State CURRENT\_NN = closest object to x from all objects associated with CURRENT\_NODE
					\State CURRENT\_NN\_DIST = distance from x to CURRENT\_NN
					\If {CURRENT\_NN\_DIST < NN\_DIST}
						\State NN = CURRENT\_NN
						\State NN\_DIST = CURRENT\_NN\_DIST
					\EndIf
					\State return NN, NN\_DIST
				\Else
				\For{each node NODE from children of CURRENT\_NODE}
					\State DIST = distance from x to rectangle of CURRENT\_NODE
					\If{NN\_DIST $\geq$ DIST}
						\State mark NODE and all its descendants as checked
					\Else
						\State NN,NN\_DIST = check\_tree(NODE,x,NN,NN\_DIST)
					\EndIf
				\EndFor
				\EndIf
		\EndProcedure

\end{algorithmic}
\end{algorithm}

\subsection{Неэффективность деревьев в пространствах высокой размерности}

В предыдущей секции были приведены примеры древовидных структур данных, которые разбивают признаковое пространство на некоторые области, с помощью его ускоряется поиск ближайших соседей. Стоит отметить, что таких структур данных на практике встречается существует огромное количество. К примеру, существующие методы можно оптимизировать, если выбирать правило для разбиения пространства не по одному конкретному признаку, а в направлении первой главной компоненты, что приведет к более сбалансированным деревьям и ускорению операции запроса \cite{balltree}. Однако было установлено, что все подобные структуры данных перестают давать преимущство в скорости выполнения запроса с ростом размерности признакового пространства. Более того, этот вопрос был детально исследован, а предыдущее утверждение было строго доказано в \cite{issues}. Приведем некоторые ключевые наблюдения из данной статьи (которые в совокупности принято называть проклятием размерности), а также основные результаты.

\begin{obs}[Число разбиений]
	Наиболее простая схема разбиения пространства делит его по каждой размерности на две части. Имея $d$-мерное пространство, будет существовать $2^d$ областей разбиения. Если $d \leq 10$ и число объектов имеет порядок около $10^6$, то в разбиениях будет смысл. Однако если $d$ растет, скажем, до 100, то число разбиений будет порядка $10^{30}$ для числа объектов $10^6$, то подавляющее большинство областей будет пустыми.  
\end{obs}

\begin{obs}[Разреженность данных в пространствах высокой размерности]
	Рассмотрим $d$ мерный единичный гиперкуб $\Omega$ в признаковом пространстве. Рассмотрим запрос получения данных из гиперкуба со стороной $l$. Тогда вероятность того, что равномерно распределенная по единичную кубу точка попадет в наш запрос равна $$P^d[s] =s^d$$ При $d=100$, $l=0.95$ эта вероятность будет равна $0.59\%$. Отметим, что меньший гиперкуб может быть расположен где угодно в $\Omega$. Отсюда можно сделать вывод, что нам сложно найти точки в $\Omega$, пространство является разреженным.
\end{obs}

\begin{obs}[Сферические запросы]
	Рассмотрим наибольший сферический запрос $sp^d(Q, 0.5)$, помещающийся в признаковое пространство с центром $Q$. Вероятность того, что произвольная точка $R$ лежит внутри этого запроса определяется отношением объемов:$$P[R\in sp^d(Q, \frac{1}{2})] = \frac{Vol(sp^d(Q, \frac{1}{2}))}{Vol{(\Omega)}} = \frac{\sqrt{\pi^d}\left(\frac{1}{2} \right)^d} {\Gamma(\frac{d}{2} + 1)} $$
	
	Если $d$ является четным числом, то это выражение можно упростить до
	$$P[R \in sp^d(Q, \frac{1}{2})] = \frac{\sqrt{\pi^d}\left( \frac{1}{2} \right) !}{\left( \frac{d}{2}\right) !}$$
	Примеры значений этой вероятности приведены во втором столбце таблицы 1.
\end{obs}

\begin{obs}[Экспоненциальный рост набора данных]
	Из значения вероятности из наблюдения 3 можно получить размер набора данных, который необходим, чтобы хотя бы одна точка в среднем попадала в запрос:$$N(d) = \frac{\left( \frac{d}{2} \right)}{ \sqrt{\pi^d} \left( \frac{1}{2} \right)^d}$$
	Некоторые значения этого количества в зависимости от $d$ приведены в третьем столбцы таблицы 1.
\end{obs}

\begin{table}[H]
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		d   & $P[R \in sp^d(Q, 0.5)]$     & $N(d)$ \\ \hline
		2   & 0.785 & 1.273 \\ \hline
		4   & 0.308 & 3.242 \\ \hline
		10  & 0.002     & 401.5 \\ \hline
		20  & $2.461 \times 10^{-8}$     & 40631627 \\ \hline
		40  & $3.278 \times 10^{-21}$     & $3.050 \times 10^{20}$ \\ \hline
		100 & $1.868 \times 10^{-70}$     & $5.353 \times 10^{69}$ \\ \hline
	\end{tabular}
\end{center}
\caption{Проклятие размерности}
\end{table}		

Исходы из этих наблюдений, а также проведя ряд других исследований, авторы статьи приходят к следующим заключениям.

\begin{conc}[Производительность]
	Для каждого метода кластеризации и разбиения, существует размерность $\widetilde{d}$, такая что на наборе данных в признаковом пространстве размерности $d > \widetilde{d}$ алгоритм прямого перебора работает быстрее.	
\end{conc}

\begin{conc}[Сложность]
	Вычислительная сложность всех алгоритмов кластеризации и разбиения стремится к $\mathcal{O}(N)$ при увеличении размерности пространства $d$.
\end{conc}

\begin{conc}[Деградация]
	Для каждого метода кластеризации и разбиения, существует размерность $\widetilde{d}$, такая что на наборе данных в признаковом пространстве размерности $d > \widetilde{d}$ в среднем будут перебраны все области разбиения.
\end{conc}

В разделе «Вычислительные эксперименты» будет показано, что на практике алгоритмы поиска ближайших соседей перестают быть эффективными (работают столько же времени, как линейный поиск, или даже медленнее его) при $d$ примерно равным 10.

\subsection{Приближенные методы}

Как было установлено в предыдущей секции, древовидные структуры данных перестают оптимизировать поиск ближайших соседей в пространствах высокой размерности. Однако на практике достаточно часто требуется работать с пространствами высокой размерности. Примерами таких задач могут служить задачи поиска похожих изображений и текстов. Весьма часто возникает необходимость поиска дубликатов среди документов в некотором наборе данных. Оказывается, что существенный прирост производительности в задаче поиска ближайших соседей можно получить, если отказаться от точного её решения и перейти к приближенному. Строго говоря, понятие «приближенное решение» не имеет общепризнанного определения, разные авторы в своих трудах могут уточнять, что именно они понимают под этим. Однако интуиция за этим стоит всегда одинаковая: имея набор данных $X$ и запрос $q$, алгоритм имеет право выдавать не самого ближайшего соседа из $X$ к $q$, а «почти ближайшего». Данное ослабление требований делается для существенного повышения скорости работы таких алгоритмов. Кроме того, можно также видеть и дополнительные возможности приближенных методов: к примеру, решая задачу поиска дубликатов среди текстов, мы можем получить «почти дубликаты», то есть тексты, которые немного отличаются, но в сущности являются почти одинаковыми. Рассмотрим классические и наиболее современные методы приближенного поиска ближайших соседей.

\subsection{Приближенные методы: LSH}

Большой класс алгоритмов приближенного поиска ближайших соседей основывается на отображении исходного признакового пространства в некоторое другое пространство, в котором проверку на схожесть выполнить проще. Такие отображения обычно называются хэш функциями, а сам процесс хэшированием. Аналогии данному процессу можно найти в области обработки естественного языка: векторы-слова, полученные с помощью One-Hot кодирования, превращаются в вектора малой размерности с помощью некоторого отображения, которое проводится таким образом, чтобы выполнялся некоторый критерий. В рассматриваемой нами задаче поиска ближайших соседей требуется найти такое отображение, чтобы близкие в некотором смысле объекты имели похожие хэши, а дальние -- достаточно разные (можно заметить, что данная идея является полной противоположность требований к хэшу в криптографии, ведь в этой области требуется, чтобы сходство хэшей не свидетельствовало о схожести исходных данных). Такое хэширование принято называть локально чувствительным хэширование (Locality-sensitive hashing, LSH). Этот алгоритм опирается на существование локально чувствительных хэшей. Приведем формальное определения этого понятия \cite{lsh}.

\begin{defin}
	Семейство $\mathcal{H}$ функций из пространства $\mathbb{X}$ в какое-то пространство $\widetilde{\mathbb{X}}$ называется $\left(R, cR, P_1, P_2 \right)$-чувствительным, если $\forall p, q \in \mathbb{X}$
	\begin{itemize}
		\item если $\Norm{p - q} \leq R$, то $\mathbb{P}_{\mathcal{H}}[h(q) = h(p)] \geq P_1$
		\item если $\Norm{p - q} \geq cR$, то $\mathbb{P}_{\mathcal{H}}[h(q) = h(p)] \leq P_2$
	\end{itemize}
\end{defin}



\noindent Чтобы такое семейство было полезным, логично потребовать также, чтобы выполнялось неравенство $P_1 > P_2$. Также обратим внимание, что вероятность берется по семейству функций $\mathcal{H}$ с равномерной вероятностной мерой.

\begin{example}
	Рассмотрим случай, когда $\mathbb{X} = \{0, 1 \}^d$ - пространство бинарных векторов размерности $d$, метрика -- расстояние Хэмминга (число компонент, в которых два вектора различны). Возьмем в качестве $\mathcal{H}$ набор функций, представляющих собой проекции различных компонент вектора: $h_i(p) = p_i, \enspace i \in \overline{1,d}$. Выбирая равномерно функцию $h_i$ из $\mathcal{H}$, мы будем получать случайную компоненту вектора $p$. Заметим, что данное семейство является локально-чувствительным: вероятность $\mathbb{P}_{\mathcal{H}}[h(q) = h(p)]$ равна доле совпадающих компонент векторов $p$ и $q$. Отсюда $P_1 = 1 - \frac{R}{d}$, $P_2 = 1 - \frac{cR}{d}$. Поскольку параметр аппроксимации $c > 1$, то $P_1 > P_2$.
\end{example}

После выбора семейства функций $\mathcal{H}$, итоговый хэш (тэг, эмбеддинг) для объекта из исходного пространства обычно получается путем конкатенации значений нескольких случайным образом выбранных (но при этом фиксированных для данного алгоритма) функций из $\mathcal{H}$. Пример 1 в сущности иллюстрирует, что обычно в качестве $\mathcal{H}$ берется набор легко вычислимых функций, удовлетворяющих определению. Чаще всего $\mathcal{H}$ выбирается исходя из метрики в пространстве $\mathbb{H}$. На практике в машинном обучении и анализе данных наиболее часто применяются евклидово расстояние и косинусное расстояние. Рассмотрим примеры семейств $\mathcal{H}$, которые используются для этих метрик \cite{stanford}.

\textbf{Косинусное расстояние.} В качестве функции $h \in $$\mathcal{H}$ обычно берется $h(x) = \text{sign} \langle w, x \rangle$, где $w$ - некоторый вектор из $\mathbb{R}^n$. При этом разным функциям $h$ соответствуют разные (но фиксированные) $w$, полученные случайным образом (компоненты векторов $w$ можно получать, к примеру, из равномерного распределения). Несложно понять, почему такое семейство функций будет попадать под определение: множество $\langle w, x \rangle = 0$ представляет собой гиперплоскость в $n$ мерном пространстве. Если две точки получили одинаковый хэш, то это значит, что они лежат по разные стороны от данной гиперплоскости. Но если угол между этими точками (мерой которого служит косинусное расстояние) достаточно мал, то вероятность того, что гиперплоскость попадет в этот небольшой промежуток, мала. Проиллюстрируем это на обычной плоскости $\mathbb{R}^2$.	

\begin{figure}[H]
	\begin{center}
		{\includegraphics[scale=0.5]{cos.jpg}}
	\end{center}
	\caption{Угол между двумя точками}
	\label{fig:image}
\end{figure}

\noindent На рисунке 4 изображены 2 синие точки. Угол между данными точками равен $\theta$. Найдем вероятность того, данные точки будут располагаться по разные стороны от случайным образом (коэффициенты выбираются из равномерного распределения) проведенной прямой. Поскольку прямая определяется углом с осью абсцисс, то, очевидно эта вероятность равна $\mathbb{P} = \frac{\theta}{\pi}$. Поскольку косинусное расстояние монотонным образом зависит от угла между двумя точками, то, применив некоторое монотонное преобразование к косинусному расстоянию, мы можем узнать точную вероятность того, что случайнным образом определенная прямая разделит точки $x_1$ и $x_2$, таких что $d_{cos}(x_1, x_2) = R$. В то же время, если между точками большой угол, то и вероятность того, что данные точки получат разные теги функцией $h(x)$, будет велика. Стоит отметить, что в этих рассуждениях мы выбирали случайным образом прямые, а не функции $h \in \mathcal{H}$, хотя в определении локально чувствительного семейства фигурирует вероятность по $\mathcal{H}$. Однако это не является ошибкой, потому что семейство $\mathcal{H}$ мы выбираем сами, случайным образом добавляя в него функции, соответствующие различным разделяющим гиперплоскостям.

\textbf{Евклидово расстояние.} Для евклидова расстояния каждая функция $h(x) \in \mathcal{H}$ соответствует прямой в многомерном пространстве $\mathbb{R}$. При этом каждая такая прямая разбивается на отрезки равной длины $a$. Для получения хэша точка $x$ сначала проецируется на данную прямую, а после этого происходит поиск номера отрезка, в который она попала. Иллюстрация:

\begin{figure}[H]
	\begin{center}
		{\includegraphics[scale=0.4]{euc.jpg}}
	\end{center}
	\caption{Хэш для евклидового расстояния}
	\label{fig:image}
\end{figure}

\noindent Функция $h(x)$, которая соответствует данной прямой, выдаст значение 4 для объекта $x$, который изображен синей точкой. Выполнять данное хэширование оказывается тоже достаточно просто: для получения проекции точки $x \in \mathbb{R}^n$ на направление $d \in \mathbb{R}^n$, где $\Norm{d} = 1$, достаточно взять посчитать их скалярное произведение: $p = \langle x, d \rangle$. Для получения отрезка разбиения, в который попадет проекция, достаточно взять (допуская волность речи) остаток от деления $p$ на $a$, где $a$ -- длина отрезков разбиений, гиперпараметр модели. Заметим, что одна такая хэш функция разбивает все пространство на бесконечные полосы равной ширины:
\begin{figure}[H]
	\begin{center}
		{\includegraphics[scale=0.37]{plane.jpg}}
	\end{center}
	\caption{Области разбиения плоскости одной хэш функцией}
	\label{fig:image}
\end{figure}

\noindent Равные значения хэшей получат те объекты $x$, которые лежат в одной такой полосе. Однако поскольку итоговая хэш функция получается конкатенацией нескольких элементов из $\mathcal{H}$, то вместе они будут разбивать плоскость на некоторые многоугольники:

\begin{figure}[H]
	\begin{center}
		{\includegraphics[scale=0.37]{plane2.jpg}}
	\end{center}
	\caption{Области разбиения плоскости двумя хэшами}
	\label{fig:image}
\end{figure}

\noindent Интуитивно понятно, семейство $\mathcal{H}$ является локально чувствительным. Строгое доказательство этого факта можно найти в \cite{stanford}.

В заключение разговора о LSH стоит также описать известный метод хэширования \textbf{MinHash} для поиска дубликоватов в наборе текстов.	Прежде чем использовать данный алгоритм, необходимо некоторым образом представить текст в виде вектора. Наиболее простым и распространенным способом сделать применительно к нашей задаче является метод разбиения документа на цепочки из k последовательно идущих слов и их one-hot кодирования (shingling). Для задачи поиска дубликатов обычно используются значения k от 5 и больше.

\begin{example}
	Кодирование строк A = «ab ba ba ab» и B = «ca ab ba» при длине цепочки k=2 происходит следующим образом: составляются пары соседних слов [«ab ba», «ba ba», «ba ab»], [«ca ab», «ab ba»]. Каждой уникальной паре сопоставляется некоторая позиция в векторе (таким образом, финальная размерность кодов будет равна числу уникальных пар слов во всем наборе документов). Сделаем следующее сопоставление: \{«ab ba» = 1, «ba ba» = 2, «ba ab» = 3, «ca ab» = 4\}. Тогда код первой строчки [1, 1, 1, 0], а второй [1, 0, 0, 1].
\end{example}

\noindent Таким образом, мы представляем каждый текст в виде множества шинглов и описываем бинарными векторами. В качестве меры сходства между такими множествами можно взять коэффициент Жаккара $$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

\noindent Легко заметить, что размерность векторов кодов для документов может быть колоссально большой, поэтому в случае, когда документов достаточно много, прямое вычисление коэффициента Жаккара между всеми парами документов оказывается очень неэффективным.

Альтернативой является хэширование данных бинарных векторов с помощью алгоритма MinHash. Кратко опишем его реализацию.

\noindent \textbf{Шаг 1.} Из кодов документов составляется матрицу Shingles $\times$ Documents.

\noindent \textbf{Шаг 2.} Строки матрицы переставляются случайным образом.

\noindent \textbf{Шаг 3.} В каждом столбце происходит поиск номера первой строки, значение в которой равно единице.

\noindent \textbf{Шаг 4.} Собрать полученные номера в вектор, добавить его к результирующей матрице H. Если число строк H меньше c, повторить шаги 2 и 3.\\



\noindent Натуральное число c является гиперпараметром алгоритма. Итоговые хэши для каждого документа будут располагаться по столбцам матрицы H. Именно они и будут сравниваться для выявления дубликатов.

Важным свойством данного алгоритма (благодаря которому он так популярен) является тот факт, что вероятность совпадения MinHash для случайной перестановки элементов двух множеств равна коэффициенту Жаккара этих множеств. Таким образом, при увеличении параметра c, мы все точнее оцениваем данную вероятность, а тем самым и J(A, B). На практике этот параметр логично выбирать таким, чтобы оценка как можно более точной, но при этом время вычисления оставалось приемлимым. Возможно также организовывать иерархию из нескольких хэшей: сначала применить MinHash, а потом какой-нибудь другой метод приближенного поиска ближайших соседей, например, LSH для евклидового расстояния. Иллюстрации к MinHash (источник \cite{minhash})

\begin{figure}[H]
	\begin{center}
		{\includegraphics[scale=0.5]{minhash2.png}}
	\end{center}
	\caption{MinHash \cite{minhash}}
	\label{fig:image}
\end{figure}

Отметим, что представленные методы хэширования не исчерпывают все существующие. К примеру, достаточно популярным является метод FlyHash\cite{flyhash}, основная идея которого была получена из области биологии, а именно из способа распозавания запахов мухой «Дрозофила фруктовая».

\subsection{Приближенные методы: FAISS}

В то время как приближенные методы поиска ближайших соседей с помощью LSH активно развивались уже долго время, наиболее эффективными на данный момент являются алгоритмы, разработанные в ближайшие 5 лет. Среди них библиотека от исследовательской группы Facebook под названием FAISS (Facebook AI Similarity Search) и графовый метод поиска ближайших соседей HNSW (Hierarchical Navigable Small World), разработанный нашими соотечественниками. В данной секции будет изложен подробный обзор библиотеки от Facebook, а в следующей будет рассмотрен HNSW.

Прежде всего стоит упомянуть статью \cite{faiss}, выпущенную авторами библиотеки, в которой излагаются подробности реализации алгоритма, а также его особенности, связанные с его выполнением на GPU. Одной из важных особенностей FAISS является тот факт, что большинство её методов могут выполняться именно на графических процессорах, которые уже достаточно давно весьма активно используются в области машинного обучения для вычислений, связанных с обучением глубоких нейронных сетей. GPU «заточены» под однородные параллельные вычисления, работу с тензорами и матрицами. Возможность выполнения алгоритмов FAISS на GPU существенно повышает эффективность их работы.

Библиотека включает в себя эффективную реализацию нескольких методов поиска ближайших соседей (как приближенных, так и точных). Среди них, к примеру, уже упомянутый HNSW. Однако наибольшую популярность FAISS завоевала благодаря эффективной реализации алгоритма IVFADC \cite{IVFADC} с использованием Product quantization (IVFPQ). Опишем его подробнее.

Основной идеей данного алгоритма является так называемая квантизация (слово широко используется в обработке сигналов), а именно представление вектора в некотором дискретном пространстве. В качестве простейшего аналога можно привести следующий пример. Имея набор данных X, мы можем откластеризовать эти точки на k кластеров, используя некоторый алгоритм кластеризации (например, K-Means). После этого мы можем заменить каждый вектор из X на центроид его кластера (или его индекс). После этого, при поступлении запроса $q$, мы можем посчитать расстояния до всех центроидов этих кластеров, и выдать из них минимальный. Проблема такого подхода заключается в trade-off между точностью результата и количеством вычислений. Очевидно, что если исходный набор данных велик (а именно таким наборам и посвящена данная работа), то, выбрав малое число кластеров, мы получим низкую точность. Однако если выбрать слишком большое число кластеров, то возрастает сложность кластеризации и время поиска ближайшего центроида для запроса. Именно эту проблему пытается решить структура IVFADC. 

Все векторы в исходном наборе данных предлагается приблизить следующим образом:$$y \approx q(y) = q_1(y) + q_2(y - q_1(y))$$
\noindent Где $q_1: \mathbb{R}^n \rightarrow \mathcal{C}_1 \subset \mathbb{R}^n$ и $q_2: \mathbb{R}^n \rightarrow \mathcal{C}_2 \subset \mathbb{R}^n$ -- это квантизаторы, то есть функции, переводящие вектора в конечные множества. При этом $q_1$ является квантизатором первого уровня и называется грубым, а $q_2$ --  квантизатор второго уровня, более точный.

Имея данные приближения, метод Asymmetric Distance Computation (ADC) выполняет поиск приближенного результата ($x$ - запрос):
$$L_{ADC} = \text{k-argmin}_{i=0:l} \Norm{x - q(y_i)}_2$$
Однако в IVFADC мы пытаемся избежать слишком большого перебора. Для этого предварительно вычисляется набор «центроидов» кластеров из множества $\mathcal{C}_1$, среди элементов которых будет происходить поиск ближайших соседей:
$$L_{IVF} = \text{$\tau$-argmin}_{c \in \mathcal{C}_1} \Norm{x-c}_2$$
\noindent В контексте приведенного выше примера, мы ищем, к каким $\tau$ центроидам кластеров из существующих ближе всех наш запрос. При этом натуральное число $\tau$ является гиперпараметром алгоритма -- он влияет на то, как много центроидов мы хотим рассматривать. После этого выполняется поиск ближайших соседей к $x$ среди тех векторов, которые относятся к кластерам, найденным на прошлом шаге:
$$L_{IVFADC} = \text{k-argmin}_{i=0:l s.t. q_1(y_i) \in L_{IVF}} \Norm{x-q(y_i)}$$
\noindent Стоит отметить, что быстрый поиск принадлежащих данному кластеру точек выполняется с помощью «инвертированного файла»: для каждого кластера составляется список индексов тех векторов, которые ему принадлежат.

Как было отмечено выше, основой данного алгоритма являются идея квантизации. Грубый квантизатор $q_1$ должен иметь сравнительно небольшое число выходных значений (авторами статьи рекомендуется использвать $|\mathcal{C}_1| \approx \sqrt{l}$), полученных с помощью K-Means. Однако $q_2$ предлагается устроить несколько более сложным образом. Проведем следующее наблюдение: поскольку в данному алгоритме мы работаем с евклидовым расстоянием $d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)}$, а функция $f(x) = \sqrt{x}$ является монотонной, то
$$\text{k-argmin}_{i=0:l} \Norm{x - y_i}_2 =  \text{k-argmin}_{i=0:l} \sqrt{\sum_{j=1}^n (x_j - y_{ij})_2} = \text{k-argmin}_{i=0:l} \sum_{j=1}^n (x_j - y_{ij})_2$$ 
\noindent Разобьем теперь размерность $n$ нашего признакового пространства на несколько равных частей: $y = [y^0 \dots y^{b-1}]$. К примеру, если мы имели вектор размерности 16, то мы можем разбить его на 4 компоненты размерности 4: $\text{[}x_1 \enspace x_2 \enspace x_3 \dots x_{15} \enspace x_{16}\text{]} \rightarrow \text{[}x_1 \enspace x_2 \enspace x_3 \enspace x_4 \text{]}, \text{[}x_5 \enspace x_6 \enspace x_7 \enspace x_8 \text{]},\text{[}x_9 \enspace x_{10} \enspace x_{11} \enspace x_{12} \text{]}, \text{[}x_{13} \enspace x_{14} \enspace x_{15} \enspace x_{16} \text{]}$. Тогда указанное выше выражение можно переписать как
$$\text{k-argmin}_{i=0:l} \sum_{j=1}^n (x_j - y_{ij})_2 = \text{k-argmin}_{i=0:l} \sum_{c=1}^C \sum_{z=1}^Z (x_{cz} - y_{icz})_2$$ 
\noindent Где $C$ - число компонент, на которые мы разбили вектор, а $Z$ - число «размерностей» в каждой из компонент. Далее, каждый из полученных подвекторов мы квантизируем с помощью отдельной (для данного номера компоненты) функции, получая кортеж $(q^0(y^0), \dots, q^{b-1}(y^{b-1}))$. Каждая из функций $q^i$ на самом деле представляет собой алгоритм K-Means с 256 кластерами (чтобы помещаться в один байт). Итоговое значение квантизации представимо в виде $q_2(y) = q^0(y^0) + 256 \times q^1(y^1) + \dots + 256 ^ {b-1} \times q^{b-1}(y^{b-1})$, что в сущности является конкатенацией байтов, полученных под-квантизаторами $q^i$. Описанный алгоритм называется Product quantizer.

В предыдущем абзаце был подробно изложен алгоритм кодирования исходных векторов в наборе данных с помощью квантизации. Опишем его ещё раз в более общих чертах:
\begin{enumerate}
\item Набор исходных данных кластеризуется с помощью K-Means на $|\mathcal{C}_1| \approx \sqrt{l}$ кластеров 
\item Для каждого вектора вычисляется разность $y - q_1(y)$ -- «невязка» между вектором и центроидом его кластера
\item Матрица полученных невязок разбивается на подпространства одинаковой размерности, и каждое из полученных подпространств кластеризуется отдельно с помощью K-Means
\item Каждый вектор из матрицы невязок кодируется последовательностью из байтов, каждый из которых соответствует индексу кластера, к которому относится соответствующая часть невязки
\end{enumerate}

\noindent Отдельно отметим, что на втором этапе квантизации используется именно невязка $y-q_1(y)$, а не сам вектор $y$, с целью повышения точности: модуль вектора $y-q_1(y)$ будет меньше, чем модуль вектора $y$, поэтому их проще и устойчивее кластеризовать.

Опишем теперь процедуру поиска ближайшего соседа. Как было указано выше, для запроса $x$ сначала нужно вычислить $\tau$ ближайших к нему центроидов кластеров. Далее мы ведем поиск ближайшего соседа среди только тех точек из X, которые принадлежат этому набору кластеров. Для каждого центроида $c_i$ мы вычисляем невязку: $\delta = x - c_i$ и должны найти среди элементов данного кластера чья невязка с $c_i$ наиболее похожа $\delta$. Однако вспомним, что все невязки закодированы последовательностью байтов - результат второго уровня квантизации. Для эффективного поиска среди этих векторов мы также разобьем запрос $x$ на подпространства и для каждого из них посчитаем расстояния до соответствующих 256 центроидов кластеров. Получим матрицу $D = C \times 256$, где $C$ - число компонент (подпространств) на которые были разбиты невязки. Теперь же, для вычисления приближенного расстояния между невязками $y_i - q_1(y_i)$ и $x - q_1(x)$, достаточно просуммировать значения матрицы $D$, взятыми по индексам столбцов, равным байтам второго уровня квантизации $q_2(y_i - q_1(y_i))$ и соответствующих им строк. То есть, для каждого номера байта $b$ к сумме добавится значение $D[b, q^b(y_i - q_1(y_i))]$. Строго говоря, таким образом мы получим оценку квадрата расстояния между $y_i - q_1(y_i)$ и $x - q_1(x)$, но выше было доказано, что точка минимума от этого не изменится.

Подведем некоторые итоги. Алгоритм IVFPQ, реализованный в библиотеке FAISS, в сущности представляет собой иерархическую систему квантизации, использующую идею разделения пространства на подпространства. Важной особенность данного алгоритма является то, что он может быть эффективно выполнен на GPU.  

\subsection{Приближенные методы: HNSW}

\section{Вычислительные эксперименты}

Цель данного раздела:
продемонстрировать, что предложенная теория работает на практике;
показать границы её применимости;
рассказать о~новых экспериментальных фактах.

Чисто теоретические работы могут вообще не~содержать раздела экспериментов
(не~работает, ну и~не~надо~--- зато теория красивая).
Кстати, теоретики имеют право не~догадываться, где, кому и~когда их теории пригодятся.

\subsection{Исходные данные и~условия эксперимента}
Описывается прикладная задача, параметры анализируемых данных 
(например, сколько объектов, сколько признаков, каких они типов), 
параметры эксперимента 
(например, как производился скользящий контроль). 

\subsection{Результаты эксперимента}
Результаты экспериментов представляются в~виде таблиц и~графиков. 
Объясняется точный смысл всех обозначений на графиках, строк и~столбцов в~таблицах. 

\subsection{Обсуждение и~выводы}
Приводятся выводы: 
в~какой степени результаты экспериментов согласуются с~теорией? 
Достигнут ли желаемый результат? 
Обнаружены ли какие-либо факты, не~нашедшие объяснения, и~которые нельзя списать на «грязный» эксперимент?

Обсуждаются основные отличия предложенных методов от известных ранее. 
В~чем их преимущества? 
Каковы границы их применимости? 
Какие проблемы удалось решить, а~какие остались открытыми? 
Какие возникли новые постановки задач?

\section{Заключение}

В~квалификационных работах последний раздел нужен для того, чтобы 
конспективно перечислить основные результаты, полученные лично автором. 

Результатами, в~частности, являются:
\begin{itemize}
\item 
    Предложен новый подход к\dots
\item 
    Разработан новый метод\dots, позволяющий\dots
\item 
    Доказан ряд теорем, подтверждающих (опровергающих), что\dots
\item 
    Проведены вычислительные эксперименты\dots,
    которые подтвердили / опровергли / привели к~новым постановкам задач.
\end{itemize}
    
Цель данного раздела: доказать квалификацию автора. 
Даже беглого взгляда на заключение должно быть достаточно, чтобы стало ясно: 
автору удалось решить актуальную, трудную, ранее не~решённую задачу, 
предложенные автором решения обоснованы и~проверены.

Иногда в~Заключении приводится список направлений дальнейших исследований.

\newpage
Список литературы необходим в~любой научной публикации. 
В дипломной работе он~обязателен. 
Дурным тоном считается:
ссылаться на работы только одного-двух авторов (например, себя или шефа);
ссылаться на слишком малое число работ;
ссылаться только на очень старые работы;
ссылаться на работы, которых автор ни разу не видел;
ссылаться на~работы, которые не~упоминаются в~тексте
или которые не~имеют отношения к~данному тексту.

\bibliography{bibliography} 
\bibliographystyle{plain}

\end{document}
